{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast\n",
    "from src.utils.decodertokens import UNMTDecoderTokens\n",
    "from src.models.Encoder import UNMTEncoder\n",
    "from src.models.Decoder import LSTM_ATTN_Decoder\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils.dataloader import UNMTDataset, data_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pretrained_type = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_pretrained_type)\n",
    "train_dataset = UNMTDataset(tokenizer = tokenizer,lang = 'hi',)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, collate_fn = lambda x: data_collate(x, tokenizer))\n",
    "\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mymode = 'Train'\n",
    "mydecoder_hidden_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myencoder = UNMTEncoder(bert_pretrained_type = bert_pretrained_type)\n",
    "myembedding_layer = myencoder.bert.embeddings.word_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_batch = [train_dataset[i] for i in range(4)]\n",
    "# print(sample_batch[3])\n",
    "# print(\"\\n\\n\\nprinted sample batch\\n\\n\\n\")\n",
    "# collated_batch = data_collate(sample_batch, tokenizer)\n",
    "# print(collated_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "input_ids, attention_mask, noisy_input_ids, noisy_attention_mask,orig_sentence, word_idx = batch\n",
    "print(input_ids.shape,\"\\n\", attention_mask.shape,\"\\n\", noisy_input_ids.shape,\"\\n\", noisy_attention_mask.shape,\"\\n\", orig_sentence,\"\\n\", word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = myencoder(noisy_input_ids,attention_mask = noisy_attention_mask)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydecoder_hi = LSTM_ATTN_Decoder(lang = 'hi', tokenizer = tokenizer, \n",
    "                                     embedding_layer = myembedding_layer, mode=mymode,\n",
    "                                     hidden_dim = mydecoder_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = mydecoder_hi(encoder_output,target_seq = input_ids, g_truth = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
